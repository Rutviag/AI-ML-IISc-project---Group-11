{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ03IaflCc40jDSpqcgMOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rutviag/AI-ML-IISc-project---Group-11/blob/main/New_file_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Reading all files and just combining complete data"
      ],
      "metadata": {
        "id": "bwdg7xboQgjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Step 1: Define the directory containing the uploaded files\n",
        "# In Colab, uploaded files are in the current working directory\n",
        "uploaded_files_dir = '.'  # Adjust this if you have a specific directory for your files\n",
        "\n",
        "# Step 2: Get all CSV files in the directory\n",
        "csv_files = [file for file in os.listdir(uploaded_files_dir) if file.endswith('.csv')]\n",
        "\n",
        "# Step 3: Combine all CSV files\n",
        "dataframes = []\n",
        "for file_name in csv_files:\n",
        "    print(f\"Reading {file_name}...\")\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Combine all DataFrames into one\n",
        "consolidated_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Step 4: Save the consolidated dataset\n",
        "output_file = 'consolidated_data.csv'\n",
        "consolidated_data.to_csv(output_file, index=False)\n",
        "print(f\"Consolidation complete. File saved as {output_file} in Colab.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmGuXjyyGs_w",
        "outputId": "ab5cbfc9-8895-4e89-eb45-9585af9b97e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 11 Nov.csv...\n",
            "Reading 08 Aug.csv...\n",
            "Reading 10 Oct.csv...\n",
            "Reading 04 April.csv...\n",
            "Reading 07 July.csv...\n",
            "Reading 12 Dec.csv...\n",
            "Reading 06 June.csv...\n",
            "Reading 01 Jan.csv...\n",
            "Reading 05 May.csv...\n",
            "Reading 09 Nov.csv...\n",
            "Reading 03 March.csv...\n",
            "Reading 02 Feb.csv...\n",
            "Consolidation complete. File saved as consolidated_data.csv in Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Getting number of line items"
      ],
      "metadata": {
        "id": "IXrcHhvpQttS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the consolidated CSV file (if not already loaded)\n",
        "df = pd.read_csv('consolidated_data.csv')  # Replace with your file name if different\n",
        "\n",
        "# Get the number of rows (line items)\n",
        "line_items_count = len(df)\n",
        "\n",
        "print(f\"The number of line items in the dataset is: {line_items_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZ8fVmAI8ZW",
        "outputId": "ce16d7e8-0f5e-4779-d02b-b11716cb105c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of line items in the dataset is: 1961484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Removing extra coumns"
      ],
      "metadata": {
        "id": "FdiHGYIWQxhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the consolidated CSV file\n",
        "df = pd.read_csv('consolidated_data.csv')  # Replace with your file name if different\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_to_remove = ['YEAR', 'DEP_TIME', 'FLIGHTS']\n",
        "df_cleaned = df.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "# Save the cleaned dataset\n",
        "output_file = 'cleaned_data.csv'\n",
        "df_cleaned.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Columns {columns_to_remove} removed. Cleaned data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw4abjcCPVTj",
        "outputId": "b5fd5d3c-5090-4f95-fa02-1e625b245895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns ['YEAR', 'DEP_TIME', 'FLIGHTS'] removed. Cleaned data saved as cleaned_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Step 4: Selecting Top 10 Cities"
      ],
      "metadata": {
        "id": "EDG2bNPzvwSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Selecting top 10 cities\n",
        "file_path = 'cleaned_data.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Get the top 10 origin cities by flight count\n",
        "top_cities = df['ORIGIN_CITY_NAME'].value_counts().head(10)\n",
        "\n",
        "# Display the top 10 cities\n",
        "print(\"\\nTop 10 Origin Cities Based on Number of Flights:\")\n",
        "print(top_cities)\n",
        "\n",
        "# Filter the dataset to include only the top 10 origin cities\n",
        "top_cities_list = top_cities.index.tolist()\n",
        "df_filtered = df[df['ORIGIN_CITY_NAME'].isin(top_cities_list)]\n",
        "\n",
        "# Save the filtered dataset\n",
        "output_file = 'filtered_data_top_10_cities.csv'\n",
        "df_filtered.to_csv(output_file, index=False)\n",
        "print(f\"\\nFiltered data with top 10 cities saved as {output_file}.\")\n",
        "\n",
        "file_path = 'filtered_data_top_10_cities.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww0upLzyvvvR",
        "outputId": "94a4c3db-b2de-4dda-c8ad-a3250c9b3111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 1961484\n",
            "\n",
            "Top 10 Origin Cities Based on Number of Flights:\n",
            "ORIGIN_CITY_NAME\n",
            "New York, NY             299738\n",
            "Chicago, IL               92985\n",
            "Atlanta, GA               74370\n",
            "Dallas/Fort Worth, TX     64575\n",
            "Denver, CO                62243\n",
            "Washington, DC            57072\n",
            "Charlotte, NC             55772\n",
            "Houston, TX               45547\n",
            "Los Angeles, CA           42980\n",
            "Orlando, FL               40981\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Filtered data with top 10 cities saved as filtered_data_top_10_cities.csv.\n",
            "Number of rows in the dataset: 836263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Cleaning the Date Format"
      ],
      "metadata": {
        "id": "Q3DHi-jBbFCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Cleaning and extracting date from FL_DATE\n",
        "file_path = 'filtered_data_top_10_cities.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure FL_DATE is treated as a string\n",
        "df['FL_DATE'] = df['FL_DATE'].astype(str)\n",
        "\n",
        "# Normalize the formats in FL_DATE\n",
        "# Handle MM-DD-YYYY and MM/DD/YYYY formats\n",
        "df['FL_DATE'] = df['FL_DATE'].str.extract(r'(\\d{2}-\\d{2}-\\d{4})|(\\d{2}/\\d{2}/\\d{4})')[0].fillna(\n",
        "    df['FL_DATE'].str.extract(r'(\\d{2}-\\d{2}-\\d{4})|(\\d{2}/\\d{2}/\\d{4})')[1]\n",
        ")\n",
        "\n",
        "# Convert FL_DATE to datetime\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'], format='%m-%d-%Y', errors='coerce').fillna(\n",
        "    pd.to_datetime(df['FL_DATE'], format='%m/%d/%Y', errors='coerce')\n",
        ")\n",
        "\n",
        "# Handle invalid dates\n",
        "df['FL_DATE'] = df['FL_DATE'].fillna(pd.Timestamp('1900-01-01'))  # Default for invalid dates\n",
        "\n",
        "# Extract Month and Date\n",
        "df['Month'] = df['FL_DATE'].dt.month\n",
        "df['Date'] = df['FL_DATE'].dt.day\n",
        "\n",
        "# Drop the original FL_DATE column\n",
        "df = df.drop(columns=['FL_DATE'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'updated_data_with_cleaned_date.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"'Month' and 'Date' extracted and 'FL_DATE' cleaned. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeezTpokblNX",
        "outputId": "5534fe88-9e62-41d1-dbe9-b0589211a983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Month' and 'Date' extracted and 'FL_DATE' cleaned. Updated data saved as updated_data_with_cleaned_date.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Weather data conversion"
      ],
      "metadata": {
        "id": "GSR5H9gxea7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'updated_data_with_cleaned_date.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the filtered dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the filtered dataset: {num_rows}\")\n",
        "\n",
        "# Define thresholds for weather delay categories\n",
        "def categorize_weather_delay(delay):\n",
        "    if pd.isna(delay) or delay == 0:  # No delay or missing data\n",
        "        return 'favorable'\n",
        "    elif delay <= 30:  # Minor delay\n",
        "        return 'risky'\n",
        "    else:  # Significant delay\n",
        "        return 'unfavorable'\n",
        "\n",
        "# Convert `WEATHER_DELAY` to categories\n",
        "df['Weather_Category'] = df['WEATHER_DELAY'].apply(categorize_weather_delay)\n",
        "\n",
        "# Remove the original `WEATHER_DELAY` column\n",
        "df = df.drop(columns=['WEATHER_DELAY'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_weather_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nWeather delay categorized and original `WEATHER_DELAY` column removed. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13xA-I_tegzW",
        "outputId": "cfe246c9-56b7-4b70-a708-3e0c964221fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the filtered dataset: 836263\n",
            "\n",
            "Weather delay categorized and original `WEATHER_DELAY` column removed. Updated data saved as filtered_data_after_weather_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) NAS delay converison"
      ],
      "metadata": {
        "id": "VTl4n5o7hbEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'filtered_data_after_weather_update.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Define thresholds for NAS delay categories\n",
        "def categorize_nas_delay(delay):\n",
        "    if pd.isna(delay) or delay == 0:  # No delay or missing data\n",
        "        return 'favorable'\n",
        "    elif delay <= 30:  # Minor delay\n",
        "        return 'risky'\n",
        "    else:  # Significant delay\n",
        "        return 'unfavorable'\n",
        "\n",
        "# Convert `NAS_DELAY` to categories\n",
        "df['NAS_Category'] = df['NAS_DELAY'].apply(categorize_nas_delay)\n",
        "\n",
        "# Remove the original `NAS_DELAY` column\n",
        "df = df.drop(columns=['NAS_DELAY'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_nas_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nNAS delay categorized and original `NAS_DELAY` column removed. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smJVrh29he5-",
        "outputId": "3be56296-7266-44db-8f0e-721ff0e0abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 836263\n",
            "\n",
            "NAS delay categorized and original `NAS_DELAY` column removed. Updated data saved as filtered_data_after_nas_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Categorize the Carrier as reliable or unreliable , remove \"LATE_AIRCRAFT_DELAY\" and \"Year\"columns"
      ],
      "metadata": {
        "id": "HD0CTxXWjk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load the dataset after NAS update\n",
        "file_path = 'filtered_data_after_nas_update.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Step 1: Calculate the historical average delay for each carrier\n",
        "carrier_avg_delay = df.groupby('OP_CARRIER_FL_NUM')['CARRIER_DELAY'].mean()\n",
        "\n",
        "# Step 2: Map historical average delay back to the original DataFrame\n",
        "df['Carrier_Avg_Delay'] = df['OP_CARRIER_FL_NUM'].map(carrier_avg_delay)\n",
        "\n",
        "# Step 3: Categorize delays based on historical averages\n",
        "def categorize_carrier_delay(row):\n",
        "    if pd.isna(row['CARRIER_DELAY']) or row['CARRIER_DELAY'] <= row['Carrier_Avg_Delay']:\n",
        "        return 'reliable'\n",
        "    else:\n",
        "        return 'unreliable'\n",
        "\n",
        "df['Carrier_Category'] = df.apply(categorize_carrier_delay, axis=1)\n",
        "\n",
        "# Step 4: Remove the `CARRIER_DELAY`, `LATE_AIRCRAFT_DELAY` columns\n",
        "df = df.drop(columns=['CARRIER_DELAY', 'Carrier_Avg_Delay', 'LATE_AIRCRAFT_DELAY', ])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_carrier_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nCarrier delay categorized as `reliable` or `unreliable`, and `CARRIER_DELAY`, and `LATE_AIRCRAFT_DELAY` columns removed.\")\n",
        "print(f\"Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8zYYP4QjzUk",
        "outputId": "78dadd93-cc5c-40aa-e7d2-00e35036b158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 836263\n",
            "\n",
            "Carrier delay categorized as `reliable` or `unreliable`, and `CARRIER_DELAY`, `LATE_AIRCRAFT_DELAY`, `YEAR`, and `FLIGHTS` columns removed.\n",
            "Updated data saved as filtered_data_after_carrier_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update codes from here"
      ],
      "metadata": {
        "id": "ComAW-sAFeZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = 'filtered_data_after_carrier_update.csv'  # Replace with your correct file path\n",
        "flight_data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert DEP_DELAY to non-negative values\n",
        "flight_data['DEP_DELAY'] = np.clip(flight_data['DEP_DELAY'], a_min=0, a_max=None)\n",
        "\n",
        "# Save the updated DataFrame back to the same CSV file\n",
        "flight_data.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"The file '{file_path}' has been updated with non-negative DEP_DELAY values.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuMot-dt4OfA",
        "outputId": "d524c911-f45e-40e9-edd5-584874fbd3ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'filtered_data_after_carrier_update.csv' has been updated with non-negative DEP_DELAY values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Upload and Read the Dataset"
      ],
      "metadata": {
        "id": "84kCT-UCNZ98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install catboost xgboost --quiet\n",
        "\n",
        "# Import models\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'filtered_data_after_carrier_update.csv'  # Update the file path as needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Preview the data\n",
        "print(\"\\nDataset Preview:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO9I5XwJNhtF",
        "outputId": "d19dad45-672e-4d85-9cf4-0c4dcda6a193"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 836263 entries, 0 to 836262\n",
            "Data columns (total 17 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   DAY_OF_WEEK        836263 non-null  int64  \n",
            " 1   OP_UNIQUE_CARRIER  836263 non-null  object \n",
            " 2   TAIL_NUM           833184 non-null  object \n",
            " 3   OP_CARRIER_FL_NUM  836263 non-null  int64  \n",
            " 4   ORIGIN_AIRPORT_ID  836263 non-null  int64  \n",
            " 5   ORIGIN_CITY_NAME   836263 non-null  object \n",
            " 6   DEST_AIRPORT_ID    836263 non-null  int64  \n",
            " 7   DEST_CITY_NAME     836263 non-null  object \n",
            " 8   CRS_DEP_TIME       836263 non-null  int64  \n",
            " 9   DEP_DELAY          820003 non-null  float64\n",
            " 10  AIR_TIME           678641 non-null  float64\n",
            " 11  DISTANCE           695221 non-null  float64\n",
            " 12  Month              836263 non-null  int64  \n",
            " 13  Date               836263 non-null  int64  \n",
            " 14  Weather_Category   836263 non-null  object \n",
            " 15  NAS_Category       836263 non-null  object \n",
            " 16  Carrier_Category   836263 non-null  object \n",
            "dtypes: float64(3), int64(7), object(7)\n",
            "memory usage: 108.5+ MB\n",
            "None\n",
            "\n",
            "Dataset Preview:\n",
            "   DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM  OP_CARRIER_FL_NUM  \\\n",
            "0            1                9E   N131EV               4975   \n",
            "1            1                9E   N131EV               5163   \n",
            "2            1                9E   N132EV               5110   \n",
            "3            1                9E   N132EV               5199   \n",
            "4            1                9E   N133EV               4908   \n",
            "\n",
            "   ORIGIN_AIRPORT_ID ORIGIN_CITY_NAME  DEST_AIRPORT_ID  DEST_CITY_NAME  \\\n",
            "0              12953     New York, NY            10994  Charleston, SC   \n",
            "1              12953     New York, NY            11042   Cleveland, OH   \n",
            "2              11057    Charlotte, NC            12478    New York, NY   \n",
            "3              12953     New York, NY            11057   Charlotte, NC   \n",
            "4              12953     New York, NY            14685    Savannah, GA   \n",
            "\n",
            "   CRS_DEP_TIME  DEP_DELAY  AIR_TIME  DISTANCE  Month  Date Weather_Category  \\\n",
            "0          1029        0.0      91.0     641.0     11     6        favorable   \n",
            "1          1629        0.0      72.0     419.0     11     6        favorable   \n",
            "2          1906        0.0      80.0     541.0     11     6        favorable   \n",
            "3          1559        0.0      86.0     544.0     11     6        favorable   \n",
            "4           900        9.0     101.0     722.0     11     6        favorable   \n",
            "\n",
            "  NAS_Category Carrier_Category  \n",
            "0    favorable         reliable  \n",
            "1    favorable         reliable  \n",
            "2    favorable         reliable  \n",
            "3    favorable         reliable  \n",
            "4    favorable         reliable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Data Cleaning"
      ],
      "metadata": {
        "id": "7Gmjq3CbOA2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Drop `TAIL_NUM` and handle missing values\n",
        "data = data.drop(columns=['TAIL_NUM'], errors='ignore')\n",
        "data = data.dropna(subset=['DEP_DELAY'])  # Drop rows with missing target variable\n",
        "columns_to_impute = ['AIR_TIME', 'DISTANCE']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
        "\n",
        "# Encode categorical features\n",
        "object_columns = data.select_dtypes(include=['object']).columns\n",
        "print(f\"Object Columns to Encode: {object_columns}\")\n",
        "\n",
        "label_encoders = {}\n",
        "for column in object_columns:\n",
        "    le = LabelEncoder()\n",
        "    data[column] = le.fit_transform(data[column].astype(str))\n",
        "    label_encoders[column] = le\n",
        "\n",
        "print(\"\\nData After Encoding:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5XUELUaOFKc",
        "outputId": "1dbab46a-fc12-43c1-ca3c-92e2c00ea005"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Columns to Encode: Index(['OP_UNIQUE_CARRIER', 'ORIGIN_CITY_NAME', 'DEST_CITY_NAME',\n",
            "       'Weather_Category', 'NAS_Category', 'Carrier_Category'],\n",
            "      dtype='object')\n",
            "\n",
            "Data After Encoding:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 820003 entries, 0 to 836262\n",
            "Data columns (total 16 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   DAY_OF_WEEK        820003 non-null  int64  \n",
            " 1   OP_UNIQUE_CARRIER  820003 non-null  int64  \n",
            " 2   OP_CARRIER_FL_NUM  820003 non-null  int64  \n",
            " 3   ORIGIN_AIRPORT_ID  820003 non-null  int64  \n",
            " 4   ORIGIN_CITY_NAME   820003 non-null  int64  \n",
            " 5   DEST_AIRPORT_ID    820003 non-null  int64  \n",
            " 6   DEST_CITY_NAME     820003 non-null  int64  \n",
            " 7   CRS_DEP_TIME       820003 non-null  int64  \n",
            " 8   DEP_DELAY          820003 non-null  float64\n",
            " 9   AIR_TIME           820003 non-null  float64\n",
            " 10  DISTANCE           820003 non-null  float64\n",
            " 11  Month              820003 non-null  int64  \n",
            " 12  Date               820003 non-null  int64  \n",
            " 13  Weather_Category   820003 non-null  int64  \n",
            " 14  NAS_Category       820003 non-null  int64  \n",
            " 15  Carrier_Category   820003 non-null  int64  \n",
            "dtypes: float64(3), int64(13)\n",
            "memory usage: 106.4 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Feature Engineering"
      ],
      "metadata": {
        "id": "OhdrtU07OLTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract hour from CRS_DEP_TIME (scheduled departure time)\n",
        "data['DEP_HOUR'] = (data['CRS_DEP_TIME'] // 100).astype(int)\n",
        "\n",
        "# Preview to confirm the DEP_HOUR column\n",
        "print(data[['CRS_DEP_TIME', 'DEP_HOUR']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRBy8YWgeUX1",
        "outputId": "254ebbdb-f25a-4a7e-f102-eba1d90c267d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   CRS_DEP_TIME  DEP_HOUR\n",
            "0          1029        10\n",
            "1          1629        16\n",
            "2          1906        19\n",
            "3          1559        15\n",
            "4           900         9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical buckets for DEP_HOUR\n",
        "def categorize_dep_hour(hour):\n",
        "    if 0 <= hour < 6:\n",
        "        return \"Night\"\n",
        "    elif 6 <= hour < 12:\n",
        "        return \"Morning\"\n",
        "    elif 12 <= hour < 18:\n",
        "        return \"Afternoon\"\n",
        "    else:\n",
        "        return \"Evening\"\n",
        "\n",
        "# Add DEP_HOUR_CATEGORY and one-hot encode it\n",
        "data['DEP_HOUR_CATEGORY'] = data['DEP_HOUR'].apply(categorize_dep_hour)\n",
        "data = pd.get_dummies(data, columns=['DEP_HOUR_CATEGORY'], drop_first=True)\n",
        "\n",
        "# Add peak hour feature\n",
        "def is_peak_hour(hour):\n",
        "    return 1 if (7 <= hour <= 10 or 16 <= hour <= 19) else 0\n",
        "\n",
        "data['IS_PEAK_HOUR'] = data['DEP_HOUR'].apply(is_peak_hour)\n",
        "\n",
        "# Add minutes since midnight\n",
        "data['MINUTES_SINCE_MIDNIGHT'] = data['DEP_HOUR'] * 60\n",
        "\n",
        "# Add interaction features\n",
        "data['DEP_HOUR_DAY_OF_WEEK'] = data['DEP_HOUR'] * data['DAY_OF_WEEK']\n",
        "data['DEP_HOUR_DISTANCE'] = data['DEP_HOUR'] * data['DISTANCE']\n",
        "\n",
        "# Preview updated dataset\n",
        "print(\"Feature Engineering Complete. Updated Dataset:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "CgP4roiTOK7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460e6db7-5c50-47f4-9403-337df5bb46e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Engineering Complete. Updated Dataset:\n",
            "   DAY_OF_WEEK  OP_UNIQUE_CARRIER  OP_CARRIER_FL_NUM  ORIGIN_AIRPORT_ID  \\\n",
            "0            1                  0               4975              12953   \n",
            "1            1                  0               5163              12953   \n",
            "2            1                  0               5110              11057   \n",
            "3            1                  0               5199              12953   \n",
            "4            1                  0               4908              12953   \n",
            "\n",
            "   ORIGIN_CITY_NAME  DEST_AIRPORT_ID  DEST_CITY_NAME  CRS_DEP_TIME  DEP_DELAY  \\\n",
            "0                 7            10994              43          1029        0.0   \n",
            "1                 7            11042              53          1629        0.0   \n",
            "2                 1            12478             192          1906        0.0   \n",
            "3                 7            11057              46          1559        0.0   \n",
            "4                 7            14685             248           900        9.0   \n",
            "\n",
            "   AIR_TIME  ...  NAS_Category  Carrier_Category  DEP_HOUR  \\\n",
            "0      91.0  ...             0                 0        10   \n",
            "1      72.0  ...             0                 0        16   \n",
            "2      80.0  ...             0                 0        19   \n",
            "3      86.0  ...             0                 0        15   \n",
            "4     101.0  ...             0                 0         9   \n",
            "\n",
            "   DEP_HOUR_CATEGORY_Evening  DEP_HOUR_CATEGORY_Morning  \\\n",
            "0                      False                       True   \n",
            "1                      False                      False   \n",
            "2                       True                      False   \n",
            "3                      False                      False   \n",
            "4                      False                       True   \n",
            "\n",
            "   DEP_HOUR_CATEGORY_Night  IS_PEAK_HOUR  MINUTES_SINCE_MIDNIGHT  \\\n",
            "0                    False             1                     600   \n",
            "1                    False             1                     960   \n",
            "2                    False             1                    1140   \n",
            "3                    False             0                     900   \n",
            "4                    False             1                     540   \n",
            "\n",
            "   DEP_HOUR_DAY_OF_WEEK  DEP_HOUR_DISTANCE  \n",
            "0                    10             6410.0  \n",
            "1                    16             6704.0  \n",
            "2                    19            10279.0  \n",
            "3                    15             8160.0  \n",
            "4                     9             6498.0  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Scale features"
      ],
      "metadata": {
        "id": "sVA-ysQCOSGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# Standardize the features (PCA works better on standardized data)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio to understand how much variance is retained\n",
        "print(\"Explained variance ratio by PCA components:\", pca.explained_variance_ratio_)\n",
        "print(\"Number of components selected:\", pca.n_components_)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = data.drop(columns=['DEP_DELAY'], errors='ignore')  # Features\n",
        "y = data['DEP_DELAY']                                  # Target variable\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Confirm no missing values in the target variable\n",
        "print(f\"Missing values in y_train: {y_train.isna().sum()}\")\n",
        "print(f\"Missing values in y_test: {y_test.isna().sum()}\")\n"
      ],
      "metadata": {
        "id": "259SoD3zOUIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074dc224-c92a-4f1c-b729-9e2309ca0ad5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in y_train: 0\n",
            "Missing values in y_test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Train Traditional Models"
      ],
      "metadata": {
        "id": "fYWNXq5DVm2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Install required libraries\n",
        "!pip install catboost xgboost --quiet\n",
        "\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"CatBoost\": CatBoostRegressor(iterations=800, learning_rate=0.1, depth=10,verbose=0),\n",
        "    \"XGBoost\": XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_true, y_pred, threshold=20):  # Threshold = Â±20 minutes\n",
        "    correct_predictions = abs(y_true - y_pred) <= threshold\n",
        "    return (correct_predictions.sum() / len(y_true)) * 100\n",
        "\n",
        "#stat=time.time()\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate performance\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    accuracy = calculate_accuracy(y_test, y_pred, threshold=20)  # Calculate accuracy\n",
        "\n",
        "    results[name] = {\"MSE\": mse, \"R2\": r2, \"Accuracy (%)\": accuracy}\n",
        "\n",
        "#print(\"Time taken for model\": )\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Performance:\")\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSyrPKTRVpY1",
        "outputId": "72485ca7-0e1d-4b04-ba23-ce2a97344b44"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "                  MSE        R2  Accuracy (%)\n",
            "CatBoost  2426.967099  0.307753     77.432206\n",
            "XGBoost   2453.242116  0.300259     77.121231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Best Model"
      ],
      "metadata": {
        "id": "gwmOzJr_GE47"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5XviSkJGEVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
