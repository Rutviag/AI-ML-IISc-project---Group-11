{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPJDbhWCbJVIT+xnR1B4zY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rutviag/AI-ML-IISc-project---Group-11/blob/main/Algos_on_new_data_with_DNN_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Reading all files and just combining complete data"
      ],
      "metadata": {
        "id": "bwdg7xboQgjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Step 1: Define the directory containing the uploaded files\n",
        "# In Colab, uploaded files are in the current working directory\n",
        "uploaded_files_dir = '.'  # Adjust this if you have a specific directory for your files\n",
        "\n",
        "# Step 2: Get all CSV files in the directory\n",
        "csv_files = [file for file in os.listdir(uploaded_files_dir) if file.endswith('.csv')]\n",
        "\n",
        "# Step 3: Combine all CSV files\n",
        "dataframes = []\n",
        "for file_name in csv_files:\n",
        "    print(f\"Reading {file_name}...\")\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Combine all DataFrames into one\n",
        "consolidated_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Step 4: Save the consolidated dataset\n",
        "output_file = 'consolidated_data.csv'\n",
        "consolidated_data.to_csv(output_file, index=False)\n",
        "print(f\"Consolidation complete. File saved as {output_file} in Colab.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmGuXjyyGs_w",
        "outputId": "ab5cbfc9-8895-4e89-eb45-9585af9b97e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 11 Nov.csv...\n",
            "Reading 08 Aug.csv...\n",
            "Reading 10 Oct.csv...\n",
            "Reading 04 April.csv...\n",
            "Reading 07 July.csv...\n",
            "Reading 12 Dec.csv...\n",
            "Reading 06 June.csv...\n",
            "Reading 01 Jan.csv...\n",
            "Reading 05 May.csv...\n",
            "Reading 09 Nov.csv...\n",
            "Reading 03 March.csv...\n",
            "Reading 02 Feb.csv...\n",
            "Consolidation complete. File saved as consolidated_data.csv in Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Getting number of line items"
      ],
      "metadata": {
        "id": "IXrcHhvpQttS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the consolidated CSV file (if not already loaded)\n",
        "df = pd.read_csv('consolidated_data.csv')  # Replace with your file name if different\n",
        "\n",
        "# Get the number of rows (line items)\n",
        "line_items_count = len(df)\n",
        "\n",
        "print(f\"The number of line items in the dataset is: {line_items_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZ8fVmAI8ZW",
        "outputId": "ce16d7e8-0f5e-4779-d02b-b11716cb105c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of line items in the dataset is: 1961484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Removing extra coumns"
      ],
      "metadata": {
        "id": "FdiHGYIWQxhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the consolidated CSV file\n",
        "df = pd.read_csv('consolidated_data.csv')  # Replace with your file name if different\n",
        "\n",
        "# Drop the specified columns\n",
        "columns_to_remove = ['YEAR', 'DEP_TIME', 'FLIGHTS']\n",
        "df_cleaned = df.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "# Save the cleaned dataset\n",
        "output_file = 'cleaned_data.csv'\n",
        "df_cleaned.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Columns {columns_to_remove} removed. Cleaned data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw4abjcCPVTj",
        "outputId": "b5fd5d3c-5090-4f95-fa02-1e625b245895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns ['YEAR', 'DEP_TIME', 'FLIGHTS'] removed. Cleaned data saved as cleaned_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Step 4: Selecting Top 10 Cities"
      ],
      "metadata": {
        "id": "EDG2bNPzvwSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Selecting top 10 cities\n",
        "file_path = 'cleaned_data.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Get the top 10 origin cities by flight count\n",
        "top_cities = df['ORIGIN_CITY_NAME'].value_counts().head(10)\n",
        "\n",
        "# Display the top 10 cities\n",
        "print(\"\\nTop 10 Origin Cities Based on Number of Flights:\")\n",
        "print(top_cities)\n",
        "\n",
        "# Filter the dataset to include only the top 10 origin cities\n",
        "top_cities_list = top_cities.index.tolist()\n",
        "df_filtered = df[df['ORIGIN_CITY_NAME'].isin(top_cities_list)]\n",
        "\n",
        "# Save the filtered dataset\n",
        "output_file = 'filtered_data_top_10_cities.csv'\n",
        "df_filtered.to_csv(output_file, index=False)\n",
        "print(f\"\\nFiltered data with top 10 cities saved as {output_file}.\")\n",
        "\n",
        "file_path = 'filtered_data_top_10_cities.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww0upLzyvvvR",
        "outputId": "94a4c3db-b2de-4dda-c8ad-a3250c9b3111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 1961484\n",
            "\n",
            "Top 10 Origin Cities Based on Number of Flights:\n",
            "ORIGIN_CITY_NAME\n",
            "New York, NY             299738\n",
            "Chicago, IL               92985\n",
            "Atlanta, GA               74370\n",
            "Dallas/Fort Worth, TX     64575\n",
            "Denver, CO                62243\n",
            "Washington, DC            57072\n",
            "Charlotte, NC             55772\n",
            "Houston, TX               45547\n",
            "Los Angeles, CA           42980\n",
            "Orlando, FL               40981\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Filtered data with top 10 cities saved as filtered_data_top_10_cities.csv.\n",
            "Number of rows in the dataset: 836263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Cleaning the Date Format"
      ],
      "metadata": {
        "id": "Q3DHi-jBbFCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Cleaning and extracting date from FL_DATE\n",
        "file_path = 'filtered_data_top_10_cities.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure FL_DATE is treated as a string\n",
        "df['FL_DATE'] = df['FL_DATE'].astype(str)\n",
        "\n",
        "# Normalize the formats in FL_DATE\n",
        "# Handle MM-DD-YYYY and MM/DD/YYYY formats\n",
        "df['FL_DATE'] = df['FL_DATE'].str.extract(r'(\\d{2}-\\d{2}-\\d{4})|(\\d{2}/\\d{2}/\\d{4})')[0].fillna(\n",
        "    df['FL_DATE'].str.extract(r'(\\d{2}-\\d{2}-\\d{4})|(\\d{2}/\\d{2}/\\d{4})')[1]\n",
        ")\n",
        "\n",
        "# Convert FL_DATE to datetime\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'], format='%m-%d-%Y', errors='coerce').fillna(\n",
        "    pd.to_datetime(df['FL_DATE'], format='%m/%d/%Y', errors='coerce')\n",
        ")\n",
        "\n",
        "# Handle invalid dates\n",
        "df['FL_DATE'] = df['FL_DATE'].fillna(pd.Timestamp('1900-01-01'))  # Default for invalid dates\n",
        "\n",
        "# Extract Month and Date\n",
        "df['Month'] = df['FL_DATE'].dt.month\n",
        "df['Date'] = df['FL_DATE'].dt.day\n",
        "\n",
        "# Drop the original FL_DATE column\n",
        "df = df.drop(columns=['FL_DATE'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'updated_data_with_cleaned_date.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"'Month' and 'Date' extracted and 'FL_DATE' cleaned. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeezTpokblNX",
        "outputId": "5534fe88-9e62-41d1-dbe9-b0589211a983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Month' and 'Date' extracted and 'FL_DATE' cleaned. Updated data saved as updated_data_with_cleaned_date.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Weather data conversion"
      ],
      "metadata": {
        "id": "GSR5H9gxea7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'updated_data_with_cleaned_date.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the filtered dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the filtered dataset: {num_rows}\")\n",
        "\n",
        "# Define thresholds for weather delay categories\n",
        "def categorize_weather_delay(delay):\n",
        "    if pd.isna(delay) or delay == 0:  # No delay or missing data\n",
        "        return 'favorable'\n",
        "    elif delay <= 30:  # Minor delay\n",
        "        return 'risky'\n",
        "    else:  # Significant delay\n",
        "        return 'unfavorable'\n",
        "\n",
        "# Convert `WEATHER_DELAY` to categories\n",
        "df['Weather_Category'] = df['WEATHER_DELAY'].apply(categorize_weather_delay)\n",
        "\n",
        "# Remove the original `WEATHER_DELAY` column\n",
        "df = df.drop(columns=['WEATHER_DELAY'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_weather_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nWeather delay categorized and original `WEATHER_DELAY` column removed. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13xA-I_tegzW",
        "outputId": "cfe246c9-56b7-4b70-a708-3e0c964221fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the filtered dataset: 836263\n",
            "\n",
            "Weather delay categorized and original `WEATHER_DELAY` column removed. Updated data saved as filtered_data_after_weather_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) NAS delay converison"
      ],
      "metadata": {
        "id": "VTl4n5o7hbEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'filtered_data_after_weather_update.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Define thresholds for NAS delay categories\n",
        "def categorize_nas_delay(delay):\n",
        "    if pd.isna(delay) or delay == 0:  # No delay or missing data\n",
        "        return 'favorable'\n",
        "    elif delay <= 30:  # Minor delay\n",
        "        return 'risky'\n",
        "    else:  # Significant delay\n",
        "        return 'unfavorable'\n",
        "\n",
        "# Convert `NAS_DELAY` to categories\n",
        "df['NAS_Category'] = df['NAS_DELAY'].apply(categorize_nas_delay)\n",
        "\n",
        "# Remove the original `NAS_DELAY` column\n",
        "df = df.drop(columns=['NAS_DELAY'])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_nas_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nNAS delay categorized and original `NAS_DELAY` column removed. Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smJVrh29he5-",
        "outputId": "3be56296-7266-44db-8f0e-721ff0e0abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 836263\n",
            "\n",
            "NAS delay categorized and original `NAS_DELAY` column removed. Updated data saved as filtered_data_after_nas_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Categorize the Carrier as reliable or unreliable , remove \"LATE_AIRCRAFT_DELAY\" and \"Year\"columns"
      ],
      "metadata": {
        "id": "HD0CTxXWjk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load the dataset after NAS update\n",
        "file_path = 'filtered_data_after_nas_update.csv'  # Replace with the file name if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows in the dataset: {num_rows}\")\n",
        "\n",
        "# Step 1: Calculate the historical average delay for each carrier\n",
        "carrier_avg_delay = df.groupby('OP_CARRIER_FL_NUM')['CARRIER_DELAY'].mean()\n",
        "\n",
        "# Step 2: Map historical average delay back to the original DataFrame\n",
        "df['Carrier_Avg_Delay'] = df['OP_CARRIER_FL_NUM'].map(carrier_avg_delay)\n",
        "\n",
        "# Step 3: Categorize delays based on historical averages\n",
        "def categorize_carrier_delay(row):\n",
        "    if pd.isna(row['CARRIER_DELAY']) or row['CARRIER_DELAY'] <= row['Carrier_Avg_Delay']:\n",
        "        return 'reliable'\n",
        "    else:\n",
        "        return 'unreliable'\n",
        "\n",
        "df['Carrier_Category'] = df.apply(categorize_carrier_delay, axis=1)\n",
        "\n",
        "# Step 4: Remove the `CARRIER_DELAY`, `LATE_AIRCRAFT_DELAY` columns\n",
        "df = df.drop(columns=['CARRIER_DELAY', 'Carrier_Avg_Delay', 'LATE_AIRCRAFT_DELAY', ])\n",
        "\n",
        "# Save the updated dataset\n",
        "output_file = 'filtered_data_after_carrier_update.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nCarrier delay categorized as `reliable` or `unreliable`, and `CARRIER_DELAY`, and `LATE_AIRCRAFT_DELAY` columns removed.\")\n",
        "print(f\"Updated data saved as {output_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8zYYP4QjzUk",
        "outputId": "78dadd93-cc5c-40aa-e7d2-00e35036b158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset: 836263\n",
            "\n",
            "Carrier delay categorized as `reliable` or `unreliable`, and `CARRIER_DELAY`, `LATE_AIRCRAFT_DELAY`, `YEAR`, and `FLIGHTS` columns removed.\n",
            "Updated data saved as filtered_data_after_carrier_update.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Upload and Read the Dataset"
      ],
      "metadata": {
        "id": "84kCT-UCNZ98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install catboost xgboost --quiet\n",
        "\n",
        "# Import models\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'filtered_data_after_carrier_update.csv'  # Update the file path as needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Preview the data\n",
        "print(\"\\nDataset Preview:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO9I5XwJNhtF",
        "outputId": "2129fe29-bb78-401f-d5d3-c3f3949cfdde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 628103 entries, 0 to 628102\n",
            "Data columns (total 17 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   DAY_OF_WEEK        628103 non-null  int64  \n",
            " 1   OP_UNIQUE_CARRIER  628103 non-null  object \n",
            " 2   TAIL_NUM           625387 non-null  object \n",
            " 3   OP_CARRIER_FL_NUM  628103 non-null  int64  \n",
            " 4   ORIGIN_AIRPORT_ID  628103 non-null  int64  \n",
            " 5   ORIGIN_CITY_NAME   628103 non-null  object \n",
            " 6   DEST_AIRPORT_ID    628103 non-null  int64  \n",
            " 7   DEST_CITY_NAME     628103 non-null  object \n",
            " 8   CRS_DEP_TIME       628103 non-null  int64  \n",
            " 9   DEP_DELAY          616058 non-null  float64\n",
            " 10  AIR_TIME           508659 non-null  float64\n",
            " 11  DISTANCE           522078 non-null  float64\n",
            " 12  Month              628103 non-null  int64  \n",
            " 13  Date               628103 non-null  int64  \n",
            " 14  Weather_Category   628103 non-null  object \n",
            " 15  NAS_Category       628103 non-null  object \n",
            " 16  Carrier_Category   628103 non-null  object \n",
            "dtypes: float64(3), int64(7), object(7)\n",
            "memory usage: 81.5+ MB\n",
            "None\n",
            "\n",
            "Dataset Preview:\n",
            "   DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM  OP_CARRIER_FL_NUM  \\\n",
            "0            1                9E   N131EV               4975   \n",
            "1            1                9E   N131EV               5163   \n",
            "2            1                9E   N132EV               5110   \n",
            "3            1                9E   N132EV               5199   \n",
            "4            1                9E   N133EV               4908   \n",
            "\n",
            "   ORIGIN_AIRPORT_ID ORIGIN_CITY_NAME  DEST_AIRPORT_ID  DEST_CITY_NAME  \\\n",
            "0              12953     New York, NY            10994  Charleston, SC   \n",
            "1              12953     New York, NY            11042   Cleveland, OH   \n",
            "2              11057    Charlotte, NC            12478    New York, NY   \n",
            "3              12953     New York, NY            11057   Charlotte, NC   \n",
            "4              12953     New York, NY            14685    Savannah, GA   \n",
            "\n",
            "   CRS_DEP_TIME  DEP_DELAY  AIR_TIME  DISTANCE  Month  Date Weather_Category  \\\n",
            "0          1029       -3.0      91.0     641.0     11     6        favorable   \n",
            "1          1629       -5.0      72.0     419.0     11     6        favorable   \n",
            "2          1906      -10.0      80.0     541.0     11     6        favorable   \n",
            "3          1559       -5.0      86.0     544.0     11     6        favorable   \n",
            "4           900        9.0     101.0     722.0     11     6        favorable   \n",
            "\n",
            "  NAS_Category Carrier_Category  \n",
            "0    favorable         reliable  \n",
            "1    favorable         reliable  \n",
            "2    favorable         reliable  \n",
            "3    favorable         reliable  \n",
            "4    favorable         reliable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Data Cleaning"
      ],
      "metadata": {
        "id": "7Gmjq3CbOA2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Drop `TAIL_NUM` and handle missing values\n",
        "data = data.drop(columns=['TAIL_NUM'], errors='ignore')\n",
        "data = data.dropna(subset=['DEP_DELAY'])  # Drop rows with missing target variable\n",
        "columns_to_impute = ['AIR_TIME', 'DISTANCE']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
        "\n",
        "# Encode categorical features\n",
        "object_columns = data.select_dtypes(include=['object']).columns\n",
        "print(f\"Object Columns to Encode: {object_columns}\")\n",
        "\n",
        "label_encoders = {}\n",
        "for column in object_columns:\n",
        "    le = LabelEncoder()\n",
        "    data[column] = le.fit_transform(data[column].astype(str))\n",
        "    label_encoders[column] = le\n",
        "\n",
        "print(\"\\nData After Encoding:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5XUELUaOFKc",
        "outputId": "6d79ee7c-c1c6-4975-c889-2f9b27d5d14d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Columns to Encode: Index(['OP_UNIQUE_CARRIER', 'ORIGIN_CITY_NAME', 'DEST_CITY_NAME',\n",
            "       'Weather_Category', 'NAS_Category', 'Carrier_Category'],\n",
            "      dtype='object')\n",
            "\n",
            "Data After Encoding:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 616058 entries, 0 to 628102\n",
            "Data columns (total 16 columns):\n",
            " #   Column             Non-Null Count   Dtype  \n",
            "---  ------             --------------   -----  \n",
            " 0   DAY_OF_WEEK        616058 non-null  int64  \n",
            " 1   OP_UNIQUE_CARRIER  616058 non-null  int64  \n",
            " 2   OP_CARRIER_FL_NUM  616058 non-null  int64  \n",
            " 3   ORIGIN_AIRPORT_ID  616058 non-null  int64  \n",
            " 4   ORIGIN_CITY_NAME   616058 non-null  int64  \n",
            " 5   DEST_AIRPORT_ID    616058 non-null  int64  \n",
            " 6   DEST_CITY_NAME     616058 non-null  int64  \n",
            " 7   CRS_DEP_TIME       616058 non-null  int64  \n",
            " 8   DEP_DELAY          616058 non-null  float64\n",
            " 9   AIR_TIME           616058 non-null  float64\n",
            " 10  DISTANCE           616058 non-null  float64\n",
            " 11  Month              616058 non-null  int64  \n",
            " 12  Date               616058 non-null  int64  \n",
            " 13  Weather_Category   616058 non-null  int64  \n",
            " 14  NAS_Category       616058 non-null  int64  \n",
            " 15  Carrier_Category   616058 non-null  int64  \n",
            "dtypes: float64(3), int64(13)\n",
            "memory usage: 79.9 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Feature Engineering"
      ],
      "metadata": {
        "id": "OhdrtU07OLTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract hour from CRS_DEP_TIME (scheduled departure time)\n",
        "data['DEP_HOUR'] = (data['CRS_DEP_TIME'] // 100).astype(int)\n",
        "\n",
        "# Preview to confirm the DEP_HOUR column\n",
        "print(data[['CRS_DEP_TIME', 'DEP_HOUR']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgP4roiTOK7g",
        "outputId": "9c3107dd-673b-4b45-b02e-248d9f0a6b6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   CRS_DEP_TIME  DEP_HOUR\n",
            "0          1029        10\n",
            "1          1629        16\n",
            "2          1906        19\n",
            "3          1559        15\n",
            "4           900         9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Scale features"
      ],
      "metadata": {
        "id": "sVA-ysQCOSGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale features\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "259SoD3zOUIz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Train Traditional Models"
      ],
      "metadata": {
        "id": "fYWNXq5DVm2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Install required libraries\n",
        "!pip install catboost xgboost --quiet\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
        "    \"XGBoost\": XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_true, y_pred, threshold=20):  # Adjust threshold to 20\n",
        "    correct_predictions = abs(y_true - y_pred) <= threshold\n",
        "    return (correct_predictions.sum() / len(y_true)) * 100\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    accuracy = calculate_accuracy(y_test, y_pred, threshold=20)\n",
        "    results[name] = {\"MSE\": mse, \"R2\": r2, \"Accuracy (%)\": accuracy}\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nTraditional Model Performance:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSyrPKTRVpY1",
        "outputId": "b9c6cdd3-e0c8-43e3-86fe-1ae4b97c950d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traditional Model Performance:\n",
            "                       MSE        R2  Accuracy (%)\n",
            "Decision Tree  5367.133041 -0.446164     69.169670\n",
            "Random Forest  2837.402958  0.235467     70.382755\n",
            "CatBoost       2545.773520  0.314046     73.885660\n",
            "XGBoost        2581.862526  0.304322     73.623781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Build and Train DNN Model"
      ],
      "metadata": {
        "id": "q_rQVS8wYnlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Build the DNN model\n",
        "dnn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "dnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = dnn_model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "dnn_mse, dnn_mae = dnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nDNN Model - MSE: {dnn_mse}, MAE: {dnn_mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmSZLRhtYxMn",
        "outputId": "d6bda2bd-c9ba-4dc5-c4ca-81317ef47054"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2ms/step - loss: 2856.5720 - mae: 22.9294 - val_loss: 2749.4126 - val_mae: 22.1296\n",
            "Epoch 2/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2819.3899 - mae: 22.6176 - val_loss: 2753.7715 - val_mae: 21.8217\n",
            "Epoch 3/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2786.6692 - mae: 22.5165 - val_loss: 2736.8586 - val_mae: 22.5241\n",
            "Epoch 4/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2715.0037 - mae: 22.4515 - val_loss: 2729.9224 - val_mae: 22.1943\n",
            "Epoch 5/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2858.6968 - mae: 22.5436 - val_loss: 2743.2126 - val_mae: 22.1572\n",
            "Epoch 6/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2846.7400 - mae: 22.5502 - val_loss: 2729.2595 - val_mae: 22.8919\n",
            "Epoch 7/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 2780.0029 - mae: 22.3515 - val_loss: 2725.7095 - val_mae: 22.2957\n",
            "Epoch 8/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2737.1843 - mae: 22.2098 - val_loss: 2748.1628 - val_mae: 23.0792\n",
            "Epoch 9/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2852.0806 - mae: 22.5270 - val_loss: 2723.1753 - val_mae: 22.4191\n",
            "Epoch 10/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2779.4929 - mae: 22.3757 - val_loss: 2726.9121 - val_mae: 21.5960\n",
            "Epoch 11/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - loss: 2707.8430 - mae: 22.3413 - val_loss: 2723.8003 - val_mae: 22.0912\n",
            "Epoch 12/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2738.3262 - mae: 22.4034 - val_loss: 2720.2407 - val_mae: 22.5266\n",
            "Epoch 13/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2725.2803 - mae: 22.2951 - val_loss: 2717.4307 - val_mae: 21.9432\n",
            "Epoch 14/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2669.9026 - mae: 22.2006 - val_loss: 2714.6199 - val_mae: 22.4887\n",
            "Epoch 15/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2723.6477 - mae: 22.2680 - val_loss: 2719.9878 - val_mae: 22.3179\n",
            "Epoch 16/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2693.9163 - mae: 22.2785 - val_loss: 2717.3669 - val_mae: 22.2635\n",
            "Epoch 17/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3ms/step - loss: 2738.9678 - mae: 22.2831 - val_loss: 2742.2056 - val_mae: 21.7531\n",
            "Epoch 18/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 2662.0703 - mae: 22.2108 - val_loss: 2713.5271 - val_mae: 22.3113\n",
            "Epoch 19/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2670.9207 - mae: 22.1632 - val_loss: 2718.3188 - val_mae: 22.5045\n",
            "Epoch 20/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2713.9453 - mae: 22.2891 - val_loss: 2738.6523 - val_mae: 21.7453\n",
            "Epoch 21/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2781.8435 - mae: 22.2992 - val_loss: 2727.3755 - val_mae: 21.8009\n",
            "Epoch 22/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2651.5405 - mae: 22.1285 - val_loss: 2731.9834 - val_mae: 22.8739\n",
            "Epoch 23/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2751.1538 - mae: 22.3393 - val_loss: 2742.8069 - val_mae: 21.6405\n",
            "Epoch 24/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 2695.7327 - mae: 22.1709 - val_loss: 2741.4326 - val_mae: 21.6549\n",
            "Epoch 25/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - loss: 2766.3098 - mae: 22.4696 - val_loss: 2736.3250 - val_mae: 21.8993\n",
            "Epoch 26/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2814.7100 - mae: 22.5016 - val_loss: 2738.5076 - val_mae: 21.9334\n",
            "Epoch 27/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2737.3677 - mae: 22.2979 - val_loss: 2754.6455 - val_mae: 21.7604\n",
            "Epoch 28/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - loss: 2668.4541 - mae: 22.2308 - val_loss: 2780.3518 - val_mae: 21.5636\n",
            "Epoch 29/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 2672.7705 - mae: 22.2085 - val_loss: 2745.8162 - val_mae: 22.0927\n",
            "Epoch 30/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2758.8848 - mae: 22.4345 - val_loss: 2771.4358 - val_mae: 21.5931\n",
            "Epoch 31/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2ms/step - loss: 2664.9971 - mae: 22.1984 - val_loss: 2772.8665 - val_mae: 21.6453\n",
            "Epoch 32/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 2696.5374 - mae: 22.3591 - val_loss: 2740.3030 - val_mae: 22.2119\n",
            "Epoch 33/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 2715.4824 - mae: 22.2978 - val_loss: 2714.9875 - val_mae: 22.1067\n",
            "Epoch 34/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 2753.8118 - mae: 22.3067 - val_loss: 2730.0601 - val_mae: 22.2695\n",
            "Epoch 35/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2696.3047 - mae: 22.1331 - val_loss: 2750.1843 - val_mae: 22.5009\n",
            "Epoch 36/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 2811.0588 - mae: 22.4868 - val_loss: 2752.4153 - val_mae: 21.8576\n",
            "Epoch 37/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 2753.5894 - mae: 22.3196 - val_loss: 2731.2881 - val_mae: 21.9887\n",
            "Epoch 38/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2703.2639 - mae: 22.1613 - val_loss: 2731.1438 - val_mae: 22.0914\n",
            "Epoch 39/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2783.8250 - mae: 22.4505 - val_loss: 2711.1426 - val_mae: 22.6129\n",
            "Epoch 40/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2682.7009 - mae: 22.3559 - val_loss: 2740.6729 - val_mae: 22.3744\n",
            "Epoch 41/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 2798.3450 - mae: 22.4743 - val_loss: 2750.9209 - val_mae: 21.9247\n",
            "Epoch 42/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 2733.2043 - mae: 22.2590 - val_loss: 2757.2734 - val_mae: 22.2247\n",
            "Epoch 43/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2757.6611 - mae: 22.3537 - val_loss: 2727.1135 - val_mae: 22.4772\n",
            "Epoch 44/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - loss: 2666.5603 - mae: 22.1717 - val_loss: 2719.3923 - val_mae: 22.6727\n",
            "Epoch 45/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 2672.6523 - mae: 22.2344 - val_loss: 2741.5767 - val_mae: 22.0066\n",
            "Epoch 46/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 2694.2651 - mae: 22.2390 - val_loss: 2760.5601 - val_mae: 21.8544\n",
            "Epoch 47/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 2703.2449 - mae: 22.1862 - val_loss: 2726.2456 - val_mae: 22.2805\n",
            "Epoch 48/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - loss: 2737.7722 - mae: 22.3174 - val_loss: 2750.8240 - val_mae: 21.8828\n",
            "Epoch 49/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 2736.7383 - mae: 22.2437 - val_loss: 2718.2444 - val_mae: 22.3637\n",
            "Epoch 50/50\n",
            "\u001b[1m10781/10781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 2741.0547 - mae: 22.4184 - val_loss: 2738.1826 - val_mae: 22.1375\n",
            "\n",
            "DNN Model - MSE: 2661.520263671875, MAE: 21.91412353515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add DNN results to the comparison\n",
        "results['DNN'] = {\"MSE\": dnn_mse, \"R2\": None, \"Accuracy (%)\": None}  # R² and Accuracy can be calculated if needed\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nAll Model Performance:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl4fZEsPg3yV",
        "outputId": "d03ed3ae-3e72-4c07-eac4-c6915c5da309"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All Model Performance:\n",
            "                       MSE        R2  Accuracy (%)\n",
            "Decision Tree  5367.133041 -0.446164     69.169670\n",
            "Random Forest  2837.402958  0.235467     70.382755\n",
            "CatBoost       2545.773520  0.314046     73.885660\n",
            "XGBoost        2581.862526  0.304322     73.623781\n",
            "DNN            2661.520264       NaN           NaN\n"
          ]
        }
      ]
    }
  ]
}